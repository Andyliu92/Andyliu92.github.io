<!doctype html><html lang=en>
<head><meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="ToD-BERT the Paper 数据集 不同的数据集可以帮助模型达到不同的熟练效果
 MetaLWOZ 预测数据 &amp;hellip;  ToD-BERT怎么训练的？  mlm contrastive function 两者都有误差，因此才可以被训练。 空间结构能够捕获差异，发现细微结构。  在服务器上运行ToD-BERT训练  进入服务器，激活环境source activate todbert_env 进入/media/HD1/dche/ToD-BERT文件夹cd /media/HD1/dche/ToD-BERT 查看GPU资源占用情况nvidia-smi，然后选择目前占用情况较低的一张GPU进行训练即可 运行训练shell脚本文件CUDA_VISIBLE_DEVICES=0 ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets。根据第三步选择的几号卡，就把对应的0改成几，此处默认单卡训练。如果一切正常的话，再读入数据集数据后，就会开始训练了，有进度条出现就Ok了。常见的没跑起来的情况是CUDA out of memory。  ToD-BERT本地调用  将ToD-BERT模型下载至本地 包含ToD-BERT所需的python包，并定义模型路径 import torch from transformers import * BERT = &amp;lt;path_to_the_downloaded_tod-bert&amp;gt; # 注意此处的路径要使用从根目录开始的绝对路径，而非从用户~目录开始的相对路径。 model_class, tokenizer_class, config_class = BertModel, BertTokenizer, BertConfig tokenizer = tokenizer_class.from_pretrained(BERT) tod_bert = model_class.from_pretrained(BERT)  使用ToD-BERT文档中的示例 # Encode text  input_text = &amp;#34;[CLS] [SYS] Hello, what can I help with you today?"><title>ToD-BERT 相关内容</title>
<link rel=canonical href=https://comfluter.life/p/tod-bert-%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/>
<link rel=stylesheet href=/scss/style.min.css><meta property="og:title" content="ToD-BERT 相关内容">
<meta property="og:description" content="ToD-BERT the Paper 数据集 不同的数据集可以帮助模型达到不同的熟练效果
 MetaLWOZ 预测数据 &amp;hellip;  ToD-BERT怎么训练的？  mlm contrastive function 两者都有误差，因此才可以被训练。 空间结构能够捕获差异，发现细微结构。  在服务器上运行ToD-BERT训练  进入服务器，激活环境source activate todbert_env 进入/media/HD1/dche/ToD-BERT文件夹cd /media/HD1/dche/ToD-BERT 查看GPU资源占用情况nvidia-smi，然后选择目前占用情况较低的一张GPU进行训练即可 运行训练shell脚本文件CUDA_VISIBLE_DEVICES=0 ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets。根据第三步选择的几号卡，就把对应的0改成几，此处默认单卡训练。如果一切正常的话，再读入数据集数据后，就会开始训练了，有进度条出现就Ok了。常见的没跑起来的情况是CUDA out of memory。  ToD-BERT本地调用  将ToD-BERT模型下载至本地 包含ToD-BERT所需的python包，并定义模型路径 import torch from transformers import * BERT = &amp;lt;path_to_the_downloaded_tod-bert&amp;gt; # 注意此处的路径要使用从根目录开始的绝对路径，而非从用户~目录开始的相对路径。 model_class, tokenizer_class, config_class = BertModel, BertTokenizer, BertConfig tokenizer = tokenizer_class.from_pretrained(BERT) tod_bert = model_class.from_pretrained(BERT)  使用ToD-BERT文档中的示例 # Encode text  input_text = &amp;#34;[CLS] [SYS] Hello, what can I help with you today?">
<meta property="og:url" content="https://comfluter.life/p/tod-bert-%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/">
<meta property="og:site_name" content="知止堂">
<meta property="og:type" content="article"><meta property="article:section" content="Post"><meta property="article:tag" content="大创 AI对话系统"><meta property="article:tag" content="机器学习"><meta property="article:published_time" content="2021-11-21T00:00:00+00:00"><meta property="article:modified_time" content="2021-11-21T00:00:00+00:00">
<meta name=twitter:title content="ToD-BERT 相关内容">
<meta name=twitter:description content="ToD-BERT the Paper 数据集 不同的数据集可以帮助模型达到不同的熟练效果
 MetaLWOZ 预测数据 &amp;hellip;  ToD-BERT怎么训练的？  mlm contrastive function 两者都有误差，因此才可以被训练。 空间结构能够捕获差异，发现细微结构。  在服务器上运行ToD-BERT训练  进入服务器，激活环境source activate todbert_env 进入/media/HD1/dche/ToD-BERT文件夹cd /media/HD1/dche/ToD-BERT 查看GPU资源占用情况nvidia-smi，然后选择目前占用情况较低的一张GPU进行训练即可 运行训练shell脚本文件CUDA_VISIBLE_DEVICES=0 ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets。根据第三步选择的几号卡，就把对应的0改成几，此处默认单卡训练。如果一切正常的话，再读入数据集数据后，就会开始训练了，有进度条出现就Ok了。常见的没跑起来的情况是CUDA out of memory。  ToD-BERT本地调用  将ToD-BERT模型下载至本地 包含ToD-BERT所需的python包，并定义模型路径 import torch from transformers import * BERT = &amp;lt;path_to_the_downloaded_tod-bert&amp;gt; # 注意此处的路径要使用从根目录开始的绝对路径，而非从用户~目录开始的相对路径。 model_class, tokenizer_class, config_class = BertModel, BertTokenizer, BertConfig tokenizer = tokenizer_class.from_pretrained(BERT) tod_bert = model_class.from_pretrained(BERT)  使用ToD-BERT文档中的示例 # Encode text  input_text = &amp;#34;[CLS] [SYS] Hello, what can I help with you today?">
<link rel="shortcut icon" href=/siteIcon/favicon.ico>
</head>
<body class="article-page has-toc">
<script>(function(){const a='StackColorScheme';localStorage.getItem(a)||localStorage.setItem(a,"auto")})()</script><script>(function(){const b='StackColorScheme',a=localStorage.getItem(b),c=window.matchMedia('(prefers-color-scheme: dark)').matches===!0;a=='dark'||a==='auto'&&c?document.documentElement.dataset.scheme='dark':document.documentElement.dataset.scheme='light'})()</script>
<div class="container main-container flex
extended">
<div id=article-toolbar>
<a href=/ class=back-home><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="15 6 9 12 15 18"/></svg>
<span>Back</span>
</a>
</div>
<main class="main full-width">
<article class=main-article>
<header class=article-header>
<div class=article-details>
<header class=article-category>
<a href=/categories/%E6%A0%BC%E8%87%B4%E4%B9%8B%E9%81%93/>
格致之道
</a>
</header>
<h2 class=article-title>
<a href=/p/tod-bert-%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/>ToD-BERT 相关内容</a>
</h2>
<footer class=article-time>
<div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 21, 2021</time>
</div>
<div><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>
2 minute read
</time>
</div>
</footer>
</div>
</header>
<section class=article-content>
<h2 id=tod-bert-the-paper>ToD-BERT the Paper</h2>
<h3 id=数据集>数据集</h3>
<p>不同的数据集可以帮助模型达到不同的熟练效果</p>
<ul>
<li>MetaLWOZ 预测数据</li>
<li>&mldr;</li>
</ul>
<h3 id=tod-bert怎么训练的>ToD-BERT怎么训练的？</h3>
<ol>
<li>mlm</li>
<li>contrastive function
两者都有误差，因此才可以被训练。
空间结构能够捕获差异，发现细微结构。</li>
</ol>
<h2 id=在服务器上运行tod-bert训练>在服务器上运行ToD-BERT训练</h2>
<ol>
<li>进入服务器，激活环境<code>source activate todbert_env</code></li>
<li>进入/media/HD1/dche/ToD-BERT文件夹<code>cd /media/HD1/dche/ToD-BERT</code></li>
<li>查看GPU资源占用情况<code>nvidia-smi</code>，然后选择目前占用情况较低的一张GPU进行训练即可</li>
<li>运行训练shell脚本文件<code>CUDA_VISIBLE_DEVICES=0 ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets</code>。根据第三步选择的几号卡，就把对应的0改成几，此处默认单卡训练。如果一切正常的话，再读入数据集数据后，就会开始训练了，有进度条出现就Ok了。常见的没跑起来的情况是CUDA out of memory。</li>
</ol>
<h2 id=tod-bert本地调用>ToD-BERT本地调用</h2>
<ol>
<li>将ToD-BERT模型下载至本地</li>
<li>包含ToD-BERT所需的python包，并定义模型路径
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>torch</span>
<span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=o>*</span>

<span class=n>BERT</span> <span class=o>=</span> <span class=o>&lt;</span><span class=n>path_to_the_downloaded_tod</span><span class=o>-</span><span class=n>bert</span><span class=o>&gt;</span>  <span class=c1># 注意此处的路径要使用从根目录开始的绝对路径，而非从用户~目录开始的相对路径。</span>
<span class=n>model_class</span><span class=p>,</span> <span class=n>tokenizer_class</span><span class=p>,</span> <span class=n>config_class</span> <span class=o>=</span> <span class=n>BertModel</span><span class=p>,</span> <span class=n>BertTokenizer</span><span class=p>,</span> <span class=n>BertConfig</span>
<span class=n>tokenizer</span> <span class=o>=</span> <span class=n>tokenizer_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>BERT</span><span class=p>)</span>
<span class=n>tod_bert</span> <span class=o>=</span> <span class=n>model_class</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>BERT</span><span class=p>)</span>
</code></pre></div></li>
<li>使用ToD-BERT文档中的示例
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=c1># Encode text </span>
<span class=n>input_text</span> <span class=o>=</span> <span class=s2>&#34;[CLS] [SYS] Hello, what can I help with you today? [USR] Find me a cheap restaurant nearby the north town.&#34;</span>
<span class=n>input_tokens</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>input_text</span><span class=p>)</span>
<span class=n>story</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>(</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>convert_tokens_to_ids</span><span class=p>(</span><span class=n>input_tokens</span><span class=p>))</span><span class=o>.</span><span class=n>long</span><span class=p>()</span>

<span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>story</span><span class=o>.</span><span class=n>size</span><span class=p>())</span> <span class=o>==</span> <span class=mi>1</span><span class=p>:</span> 
    <span class=n>story</span> <span class=o>=</span> <span class=n>story</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span> <span class=c1># batch size dimension</span>

<span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>():</span> 
    <span class=n>tod_bert</span> <span class=o>=</span> <span class=n>tod_bert</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
    <span class=n>story</span> <span class=o>=</span> <span class=n>story</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>

<span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
    <span class=n>input_context</span> <span class=o>=</span> <span class=p>{</span><span class=s2>&#34;input_ids&#34;</span><span class=p>:</span> <span class=n>story</span><span class=p>,</span> <span class=s2>&#34;attention_mask&#34;</span><span class=p>:</span> <span class=p>(</span><span class=n>story</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>long</span><span class=p>()}</span>
    <span class=n>hiddens</span> <span class=o>=</span> <span class=n>tod_bert</span><span class=p>(</span><span class=o>**</span><span class=n>input_context</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span> 
</code></pre></div></li>
</ol>
<h2 id=计算tod-bert推理时间延迟>计算ToD-BERT推理时间延迟</h2>
<h3 id=如何正确地计算>如何正确地计算</h3>
<p><a class=link href=https://www.jianshu.com/p/9b9d6782f90b target=_blank rel=noopener>深度学习中如何正确地measure inference time</a></p>
<p>问题：</p>
<ol>
<li>在进行多batch训练或推理的时候，batch1被送进GPU后，CPU由于异步执行，不再等待batch1在GPU内执行完毕，而是直接对batch2进行预处理，此时若使用python的time库，停止计算时间的代码将在GPU执行完毕前被执行，导致时长计算错误。</li>
<li>GPU在不工作时将关掉许多硬件模块，在调用GPU时需要重新初始化（GPU预热），占用大量时间，导致时间测算错误。</li>
</ol>
<p>解决方法：</p>
<ol>
<li>在真正需要的example前运行几个example，使得GPU不再处于省电模式。</li>
<li>使用<code>tr.cuda.event</code>，在GPU上测量时间</li>
<li>使用函数<code>torch.cuda.synchronize()</code>，使得CPU和GPU工作在同步执行模式。</li>
</ol>
<h3 id=在服务器上进行inference并计算inference时间>在服务器上进行inference并计算inference时间</h3>
<ol>
<li>
<p>在<code>run_tod_lm_pretraining.sh</code>文件中修改<code>batch size = 1</code>:</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=nv>gpu</span><span class=o>=</span><span class=nv>$1</span>
<span class=nv>model_type</span><span class=o>=</span><span class=nv>$2</span>
<span class=nv>bert_dir</span><span class=o>=</span><span class=nv>$3</span>
<span class=nv>output_dir</span><span class=o>=</span><span class=nv>$4</span>
<span class=nv>add1</span><span class=o>=</span><span class=nv>$5</span>
<span class=nv>add2</span><span class=o>=</span><span class=nv>$6</span>
<span class=nv>add3</span><span class=o>=</span><span class=nv>$7</span>
<span class=nv>add4</span><span class=o>=</span><span class=nv>$8</span>
<span class=nv>add5</span><span class=o>=</span><span class=nv>$9</span>

<span class=c1># ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn</span>
<span class=c1># ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-JNT --only_last_turn --add_rs_loss</span>

<span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span><span class=m>3</span> python my_tod_pretraining.py <span class=se>\
</span><span class=se></span>    --task<span class=o>=</span>usdl <span class=se>\
</span><span class=se></span>    --model_type<span class=o>=</span><span class=si>${</span><span class=nv>model_type</span><span class=si>}</span> <span class=se>\
</span><span class=se></span>    --model_name_or_path<span class=o>=</span><span class=si>${</span><span class=nv>bert_dir</span><span class=si>}</span> <span class=se>\
</span><span class=se></span>    --output_dir<span class=o>=</span><span class=si>${</span><span class=nv>output_dir</span><span class=si>}</span> <span class=se>\
</span><span class=se></span>    --do_train <span class=se>\
</span><span class=se></span>    --do_eval <span class=se>\
</span><span class=se></span>    --mlm <span class=se>\
</span><span class=se></span>    --do_lower_case <span class=se>\
</span><span class=se></span>    --evaluate_during_training <span class=se>\
</span><span class=se></span>    --save_steps<span class=o>=</span><span class=m>2500</span> --logging_steps<span class=o>=</span><span class=m>1000</span> <span class=se>\
</span><span class=se></span>    --per_gpu_train_batch_size<span class=o>=</span><span class=m>1</span> --per_gpu_eval_batch_size<span class=o>=</span><span class=m>1</span> <span class=se>\
</span><span class=se></span>    <span class=si>${</span><span class=nv>add1</span><span class=si>}</span> <span class=si>${</span><span class=nv>add2</span><span class=si>}</span> <span class=si>${</span><span class=nv>add3</span><span class=si>}</span> <span class=si>${</span><span class=nv>add4</span><span class=si>}</span> <span class=si>${</span><span class=nv>add5</span><span class=si>}</span>
</code></pre></div></li>
<li>
<p>使用上文办法，在<code>my_tod_pretraining.py</code>中引入计时相关语句：</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=c1>## with only MLM loss</span>
<span class=k>else</span><span class=p>:</span> 
    <span class=n>starter</span><span class=p>,</span> <span class=n>ender</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>Event</span><span class=p>(</span><span class=n>enable_timing</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>Event</span><span class=p>(</span><span class=n>enable_timing</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>

     <span class=n>inputs</span> <span class=o>=</span> <span class=n>batch</span><span class=p>[</span><span class=s2>&#34;context&#34;</span><span class=p>]</span><span class=o>.</span><span class=n>clone</span><span class=p>()</span>
    <span class=k>if</span> <span class=n>args</span><span class=o>.</span><span class=n>mlm</span><span class=p>:</span>
        <span class=n>inputs</span><span class=p>,</span> <span class=n>labels</span> <span class=o>=</span> <span class=n>mask_tokens</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>args</span><span class=p>)</span>
        <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
        <span class=n>labels</span> <span class=o>=</span> <span class=n>labels</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>args</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>

        <span class=n>starter</span><span class=o>.</span><span class=n>record</span><span class=p>()</span>

        <span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span>
                        <span class=n>masked_lm_labels</span><span class=o>=</span><span class=n>labels</span><span class=p>,</span>
                        <span class=n>attention_mask</span><span class=o>=</span><span class=n>inputs</span><span class=o>&gt;</span><span class=mi>0</span><span class=p>)</span>

        <span class=n>ender</span><span class=o>.</span><span class=n>record</span><span class=p>()</span>
        <span class=c1># WAIT FOR GPU SYNC</span>
        <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
        <span class=n>curr_time</span> <span class=o>=</span> <span class=n>starter</span><span class=o>.</span><span class=n>elapsed_time</span><span class=p>(</span><span class=n>ender</span><span class=p>)</span>
        <span class=nb>print</span><span class=p>(</span><span class=n>curr_time</span><span class=p>)</span>
</code></pre></div></li>
<li>
<p>由于训练时间较长，使用<code>tmux</code>命令：<code>tmux new -s inference_time_measure</code>，进入tmux回话后还需要重新激活虚拟环境。</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=se>\(</span>base<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ <span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span><span class=m>0</span> ./run_tod_lm_pretraining.sh <span class=m>0</span> bert bert-base-uncased save/prtrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets
Traceback <span class=o>(</span>most recent call last<span class=o>)</span>:
  File <span class=s2>&#34;/media/HD1/dche/ToD-BERT/my_tod_pretraining.py&#34;</span>, line 16, in &lt;module&gt;
    import numpy as np
ModuleNotFoundError: No module named <span class=s1>&#39;numpy&#39;</span>
<span class=o>(</span>base<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ conda info --env
<span class=c1># conda environments:</span>
<span class=c1>#</span>
base                  *  /media/HD1/dche/miniconda3
sum_env                  /media/HD1/dche/miniconda3/envs/sum_env
tod_bert                 /media/HD1/dche/miniconda3/envs/tod_bert
                         /media/HD1/miniconda3

<span class=o>(</span>base<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ conda activate tod_bert
<span class=o>(</span>tod_bert<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ <span class=nv>CUDA_VISIBLE_DEVICES</span><span class=o>=</span><span class=m>0</span> ./run_tod_lm_pretraining.sh <span class=m>0</span> bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets
</code></pre></div></li>
<li>
<p>进行训练，观察输出结果 <code>CUDA_VISIBLE_DEVICES=0 ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets</code></p>
</li>
<li>
<p>训练过程中可以使用<code>crtl+b d</code>从会话中分离</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>(</span>tod_bert<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ tmux new -s inference_time_measure
<span class=o>[</span>detached <span class=o>(</span>from session inference_time_measure<span class=o>)]</span>
<span class=o>(</span>tod_bert<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ 
</code></pre></div></li>
<li>
<p>可以查看当前的tmux会话并连接</p>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>(</span>tod_bert<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ tmux ls
inference_time_measure: <span class=m>1</span> windows <span class=o>(</span>created Sun Nov <span class=m>21</span> 23:26:48 2021<span class=o>)</span> <span class=o>[</span>134x33<span class=o>]</span>
zym1: <span class=m>1</span> windows <span class=o>(</span>created Sun Nov <span class=m>21</span> 18:34:03 2021<span class=o>)</span> <span class=o>[</span>148x45<span class=o>]</span>
zym2: <span class=m>1</span> windows <span class=o>(</span>created Sun Nov <span class=m>21</span> 18:34:44 2021<span class=o>)</span> <span class=o>[</span>113x12<span class=o>]</span>
<span class=o>(</span>tod_bert<span class=o>)</span> dialogue@amax-13:/media/HD1/dche/ToD-BERT$ tmux attach -t inference_time_measure
</code></pre></div></li>
<li>
<p>为了便于记录inference time，可以将bash命令中的输出全部写入txt文件，<code>script -a 1.txt</code>，则之后shell中所有文字都将被记录在<code>1.txt</code>中。</p>
</li>
</ol>
</section>
<footer class=article-footer>
<section class=article-tags>
<a href=/tags/%E5%A4%A7%E5%88%9B-ai%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/>大创 AI对话系统</a>
<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>
</section>
</footer>
</article>
<aside class=related-contents--wrapper>
<h2 class=section-title>Related contents</h2>
<div class=related-contents>
<div class="flex article-list--tile">
<article>
<a href=/p/%E5%A4%A7%E5%88%9B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F21.12.06%E7%BB%84%E4%BC%9A/>
<div class=article-details>
<h2 class=article-title>大创对话系统21.12.06组会</h2>
</div>
</a>
</article>
<article>
<a href=/p/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AEcpu%E7%89%88tod-bert%E7%8E%AF%E5%A2%83/>
<div class=article-details>
<h2 class=article-title>服务器配置CPU版ToD-BERT环境</h2>
</div>
</a>
</article>
<article>
<a href=/p/%E5%A4%A7%E5%88%9B%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F21.11.22%E7%BB%84%E4%BC%9A/>
<div class=article-details>
<h2 class=article-title>大创对话系统21.11.22组会</h2>
</div>
</a>
</article>
</div>
</div>
</aside>
<div id=gitalk-container></div>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css>
<script src=https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/blueimp-md5@2.18.0/js/md5.min.js></script>
<script>const gitalk=new Gitalk({clientID:"0d5bb28ba48d7571aaae",clientSecret:"5b4b5a48ddaf1390937e89e0f88e1deede49e4f5",repo:"personal-blog-comment",owner:"Andyliu92",admin:["Andyliu92"],distractionFreeMode:!1,id:md5(location.pathname)});(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("gitalk-container").innerHTML="Gitalk comments not available by default when the website is previewed locally.";return}gitalk.render("gitalk-container")})()</script>
<footer class=site-footer>
<section class=copyright>
&copy;
2021 知止堂
</section>
<section class=powerby>
京ICP备2021035515号 <br>
Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> <br>
Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.5.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a>
</section>
</footer>
<div class=pswp tabindex=-1 role=dialog aria-hidden=true>
<div class=pswp__bg></div>
<div class=pswp__scroll-wrap>
<div class=pswp__container>
<div class=pswp__item></div>
<div class=pswp__item></div>
<div class=pswp__item></div>
</div>
<div class="pswp__ui pswp__ui--hidden">
<div class=pswp__top-bar>
<div class=pswp__counter></div>
<button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
<div class=pswp__preloader>
<div class=pswp__preloader__icn>
<div class=pswp__preloader__cut>
<div class=pswp__preloader__donut></div>
</div>
</div>
</div>
</div>
<div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
<div class=pswp__share-tooltip></div>
</div>
<button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
</button>
<div class=pswp__caption>
<div class=pswp__caption__center></div>
</div>
</div>
</div>
</div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css integrity="sha256-c0uckgykQ9v5k+IqViZOZKc47Jn7KQil4/MP3ySA3F8=" crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE=" crossorigin=anonymous>
</main>
<aside class="sidebar right-sidebar sticky">
<section class="widget archives">
<div class=widget-icon><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
</div>
<h2 class="widget-title section-title">Table of contents</h2>
<div class=widget--toc>
<nav id=TableOfContents>
<ol>
<li><a href=#tod-bert-the-paper>ToD-BERT the Paper</a>
<ol>
<li><a href=#数据集>数据集</a></li>
<li><a href=#tod-bert怎么训练的>ToD-BERT怎么训练的？</a></li>
</ol>
</li>
<li><a href=#在服务器上运行tod-bert训练>在服务器上运行ToD-BERT训练</a></li>
<li><a href=#tod-bert本地调用>ToD-BERT本地调用</a></li>
<li><a href=#计算tod-bert推理时间延迟>计算ToD-BERT推理时间延迟</a>
<ol>
<li><a href=#如何正确地计算>如何正确地计算</a></li>
<li><a href=#在服务器上进行inference并计算inference时间>在服务器上进行inference并计算inference时间</a></li>
</ol>
</li>
</ol>
</nav>
</div>
</section>
</aside>
</div>
<script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g=" crossorigin=anonymous defer></script><script type=text/javascript src=/ts/main.js defer></script>
<script>(function(){const a=document.createElement('link');a.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",a.type="text/css",a.rel="stylesheet",document.head.appendChild(a)})()</script>
</body>
</html>