<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Early-Exit BERT on 知止堂</title><link>https://comfluter.life/tags/early-exit-bert/</link><description>Recent content in Early-Exit BERT on 知止堂</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 08 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://comfluter.life/tags/early-exit-bert/index.xml" rel="self" type="application/rss+xml"/><item><title>Early-Exit BERT 组会记录</title><link>https://comfluter.life/p/early-exit-bert-%E7%BB%84%E4%BC%9A%E8%AE%B0%E5%BD%95/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://comfluter.life/p/early-exit-bert-%E7%BB%84%E4%BC%9A%E8%AE%B0%E5%BD%95/</guid><description>2022.02.07 组会 简单讨论了一下我对于Early-Exit BERT的理解，解答了一些问题，确定了下一步代码层面的工作内容。
2月7日和刘时宜讨论：early-exit
周晨晨意见：把FSA的各个模型，让刘时宜学弟先尝试用early-exit加速某一个，然后推广到其他模型；在服务器上，新建一个目录开始尝试。
周晨晨问题：Huggingface-Transformer版本问题；
谢克力意见：先简单复现FastBERT的开源代码，再类比，把early-eixt的方法推广；
刘时宜的工作顺序：
简单复现FastBERT的开源代码，深入了解一下early-exit的具体操作，可能还包含了蒸馏的操作 复现完成之后，我们再看如何推广到我们现有的FSA模型上。 周晨晨的工作顺序：
先尝试一下能否在distillBERT上做early-exit。</description></item><item><title>FastBERT</title><link>https://comfluter.life/p/fastbert/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://comfluter.life/p/fastbert/</guid><description>Install on Surface Pro 7 Download and config FastBERT FastBERT git rep
git clone https://github.com/autoliuweijie/FastBERT.git conda create -n fastbert python=3.8.2 conda activate fastbert cd .\FastBERT\ conda install pytorch==1.5.1 torchvision==0.6.1 cpuonly -c pytorch pip install -r requirements.txt Quick start on the Chinese Book review dataset Download the pre-trained Chinese BERT parameters from here, and save it to the models directory with the name of &amp;ldquo;Chinese_base_model.bin&amp;rdquo;. Install on server with CUDA git clone https://github.</description></item></channel></rss>