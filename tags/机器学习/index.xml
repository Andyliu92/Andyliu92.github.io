<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>机器学习 on 知止堂</title><link>https://blog.comfluter.life/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 机器学习 on 知止堂</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 08 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.comfluter.life/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>ConvLab-2 Toolkit</title><link>https://blog.comfluter.life/p/convlab-2-toolkit/</link><pubDate>Tue, 08 Feb 2022 00:00:00 +0000</pubDate><guid>https://blog.comfluter.life/p/convlab-2-toolkit/</guid><description>Download toolkit git clone https://github.com/mozilla/TTS.git
Install and config env conda create -n convlab2 python=3.6.2 cd ConvLab-2 install cpu version of pytorch (run on personal computer)conda install pytorch==1.5.1 torchvision==0.6.1 cpuonly -c pytorch otherwise, auto installation cannot find package version. pip install -e .</description></item><item><title>Debugging ToD-BERT with vscode</title><link>https://blog.comfluter.life/p/debugging-tod-bert-with-vscode/</link><pubDate>Thu, 09 Dec 2021 00:00:00 +0000</pubDate><guid>https://blog.comfluter.life/p/debugging-tod-bert-with-vscode/</guid><description>Configuring launch.json in vscode &amp;#34;configurations&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;Python: my tod training&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;python&amp;#34;, &amp;#34;request&amp;#34;: &amp;#34;launch&amp;#34;, &amp;#34;program&amp;#34;: &amp;#34;${file}&amp;#34;, &amp;#34;console&amp;#34;: &amp;#34;integratedTerminal&amp;#34;, &amp;#34;args&amp;#34;: [ &amp;#34;--task=usdl&amp;#34;, &amp;#34;--model_type=bert&amp;#34;, &amp;#34;--model_name_or_path=bert-base-uncased&amp;#34;, &amp;#34;--output_dir=&amp;#34;, &amp;#34;--do_train&amp;#34;, &amp;#34;--do_eval&amp;#34;, &amp;#34;--mlm&amp;#34;, &amp;#34;--do_lower_case&amp;#34;, &amp;#34;--evaluate_during_training&amp;#34;, &amp;#34;--save_steps=2500&amp;#34;, &amp;#34;--logging_steps=1000&amp;#34;, &amp;#34;--per_gpu_train_batch_size=1&amp;#34;, &amp;#34;--per_gpu_eval_batch_size=1&amp;#34;, &amp;#34;--only_last_turn&amp;#34; ] } ] Running Debug Session open my_tod_pretraining.py and press F5 to start a debug session using the configuration shown above.
bug report:
initial bug report
initial bug console report</description></item><item><title>ToD-BERT 相关内容</title><link>https://blog.comfluter.life/p/tod-bert-%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><guid>https://blog.comfluter.life/p/tod-bert-%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/</guid><description>ToD-BERT the Paper 数据集 不同的数据集可以帮助模型达到不同的熟练效果
MetaLWOZ 预测数据 &amp;hellip; ToD-BERT怎么训练的？ mlm contrastive function 两者都有误差，因此才可以被训练。 空间结构能够捕获差异，发现细微结构。 在服务器上运行ToD-BERT训练 进入服务器，激活环境source activate todbert_env 进入/media/HD1/dche/ToD-BERT文件夹cd /media/HD1/dche/ToD-BERT 查看GPU资源占用情况nvidia-smi，然后选择目前占用情况较低的一张GPU进行训练即可 运行训练shell脚本文件CUDA_VISIBLE_DEVICES=0 ./run_tod_lm_pretraining.sh 0 bert bert-base-uncased save/pretrain/ToD-BERT-MLM --only_last_turn --data_path ./../dialog_datasets。根据第三步选择的几号卡，就把对应的0改成几，此处默认单卡训练。如果一切正常的话，再读入数据集数据后，就会开始训练了，有进度条出现就Ok了。常见的没跑起来的情况是CUDA out of memory。 ToD-BERT本地调用 将ToD-BERT模型下载至本地 包含ToD-BERT所需的python包，并定义模型路径 import torch from transformers import * BERT = &amp;lt;path_to_the_downloaded_tod-bert&amp;gt; # 注意此处的路径要使用从根目录开始的绝对路径，而非从用户~目录开始的相对路径。 model_class, tokenizer_class, config_class = BertModel, BertTokenizer, BertConfig tokenizer = tokenizer_class.from_pretrained(BERT) tod_bert = model_class.from_pretrained(BERT) 使用ToD-BERT文档中的示例 # Encode text input_text = &amp;#34;[CLS] [SYS] Hello, what can I help with you today?</description></item></channel></rss>